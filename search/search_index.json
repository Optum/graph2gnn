{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Graph2GNN \u00b6 Graph2GNN is a library that helps you prepare data from a graph database for use in training graph-based machine learning models. Example of a GCN from Ying et al., 2018 1 Overview \u00b6 Graph2GNN in 3 sentences... Write a query (with the guidance in the docs) to get data in your graph. Graph2GNN will take care of extracting the data in a way that separates the data from the graph's structure. Perform EDA and prepare the vectors, then recombine it with the graph-structure files for use in graph-based ML (not only GNNs). To see in-depth walkthroughs, check out the examples A simple node-classification example \u00b6 To get the data out of Tigergraph, write a query that follows this format: CREATE QUERY cora_data () FOR GRAPH CoraGraph { SetAccum < Edge > @@edges ; PaperSrc = { Paper .*} ; PaperResult = SELECT s FROM PaperSrc : s -( _ : e )- > Paper : tgt ACCUM @@edges += e ; PRINT @@edges ; PRINT PaperSrc ; } The query can be as simple or complex as need be. For getting one subgraph out, you need to print the @@edges SetAccum and a vertex set. Additional techniques for graph classification are described in the documentation, here . Once the query is done, move to your python code. from graph2gnn import Tiger2GNN class Cora2GNN ( Tiger2GNN ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) # create the config object tg = Tiger2GNN ( host = 'http://your-graph-host' , graph_name = 'CoraGraph' , token = '<bearer token>' , query = 'cora_data' ) tg . call_singlegraph_query () # Calls the query and organizes the data tg . compute_vertex_vectors () # *Use the patterns in the docs to complete this* tg . assemble_data () # *Use the patterns in the docs to complete this* Turning your raw data into vectors and determining the labels are something G2GNN expects you to do yourself, similar to other ML libraries. Now you're ready to move to preparing the data and training your GNN! Here's the state of your current directory and where your vectors and adjacency info are stored: project-root \u2514\u2500\u2500 tgresponse/ \u251c\u2500\u2500 subgraph/ (N/A for queries that return one graph) \u251c\u2500\u2500 parts/ \u2502 \u2514\u2500\u2500 partition_{n}.json (whole query responses, saved by partition) \u251c\u2500\u2500 VertexType.json (vertex set response... create vectors from these files) \u251c\u2500\u2500 edges.csv *(adjacency info... will become a model input) \u2514\u2500\u2500 your_app.py (or .ipynb) Again, for an in-depth walkthough, check out the examples . Installation \u00b6 Install from the source: python3 -m venv venv source venv/bin/activate cd graph2gnn/src python setup.py install Currently supported graph DBs \u00b6 Tigergraph Graph Convolutional Neural Networks for Web-Scale Recommender Systems \u21a9","title":"graph2gnn"},{"location":"#graph2gnn","text":"Graph2GNN is a library that helps you prepare data from a graph database for use in training graph-based machine learning models. Example of a GCN from Ying et al., 2018 1","title":"Graph2GNN"},{"location":"#overview","text":"Graph2GNN in 3 sentences... Write a query (with the guidance in the docs) to get data in your graph. Graph2GNN will take care of extracting the data in a way that separates the data from the graph's structure. Perform EDA and prepare the vectors, then recombine it with the graph-structure files for use in graph-based ML (not only GNNs). To see in-depth walkthroughs, check out the examples","title":"Overview"},{"location":"#a-simple-node-classification-example","text":"To get the data out of Tigergraph, write a query that follows this format: CREATE QUERY cora_data () FOR GRAPH CoraGraph { SetAccum < Edge > @@edges ; PaperSrc = { Paper .*} ; PaperResult = SELECT s FROM PaperSrc : s -( _ : e )- > Paper : tgt ACCUM @@edges += e ; PRINT @@edges ; PRINT PaperSrc ; } The query can be as simple or complex as need be. For getting one subgraph out, you need to print the @@edges SetAccum and a vertex set. Additional techniques for graph classification are described in the documentation, here . Once the query is done, move to your python code. from graph2gnn import Tiger2GNN class Cora2GNN ( Tiger2GNN ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) # create the config object tg = Tiger2GNN ( host = 'http://your-graph-host' , graph_name = 'CoraGraph' , token = '<bearer token>' , query = 'cora_data' ) tg . call_singlegraph_query () # Calls the query and organizes the data tg . compute_vertex_vectors () # *Use the patterns in the docs to complete this* tg . assemble_data () # *Use the patterns in the docs to complete this* Turning your raw data into vectors and determining the labels are something G2GNN expects you to do yourself, similar to other ML libraries. Now you're ready to move to preparing the data and training your GNN! Here's the state of your current directory and where your vectors and adjacency info are stored: project-root \u2514\u2500\u2500 tgresponse/ \u251c\u2500\u2500 subgraph/ (N/A for queries that return one graph) \u251c\u2500\u2500 parts/ \u2502 \u2514\u2500\u2500 partition_{n}.json (whole query responses, saved by partition) \u251c\u2500\u2500 VertexType.json (vertex set response... create vectors from these files) \u251c\u2500\u2500 edges.csv *(adjacency info... will become a model input) \u2514\u2500\u2500 your_app.py (or .ipynb) Again, for an in-depth walkthough, check out the examples .","title":"A simple node-classification example"},{"location":"#installation","text":"Install from the source: python3 -m venv venv source venv/bin/activate cd graph2gnn/src python setup.py install","title":"Installation"},{"location":"#currently-supported-graph-dbs","text":"Tigergraph Graph Convolutional Neural Networks for Web-Scale Recommender Systems \u21a9","title":"Currently supported graph DBs"},{"location":"about/about/","text":"Graph2GNN is an Open Source project available on Github under the Apache 2.0 license. The framework is maintained by Robert Rossmiller ( LinkedIn , email , Optum email )","title":"About"},{"location":"documentation/query/","text":"Notes on writing the query for different tasks","title":"Query"},{"location":"documentation/subclassing/","text":"Notes on writing the code for subclassing for different tasks","title":"Subclassing"},{"location":"examples/discrete-graph/","text":"Discrete Graph - Regression \u00b6 Overview \u00b6 Graph-level regression on the QM7b dataset from TigerGraph using g2gnn. QM7b is a dataset for multitask learning. Since this is just an example, only one of those tasks is trained. The dataset is a homogeneous graph of 7,211 separate molucules where each vertex is one atom and each edge is a bond. The example code is HERE . Details are below. Write the query \u00b6 The query, found at the bottom of setup.gsql . It's responsible for gathering the graphy info that the model will train on. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 CREATE QUERY QM7bData ( INT total_partitions = 0 , INT current_partition ) FOR GRAPH QM7b { SetAccum < Edge > @ edges ; MaxAccum < VERTEX > @ lead_node ; AtomSrc = { Atom . * } ; AtomResult = SELECT s FROM AtomSrc : s - ( _ : e ) -> Atom : t ACCUM s . @ edges += e , IF s . label . size () > 0 THEN // Then it is the leader node s . @ lead_node += s , t . @ lead_node += s END ; AtomResult = SELECT s FROM AtomResult : s WHERE vertex_to_int ( s ) % total_partitions == current_partition ; PRINT AtomResult ; } Lines 2 and 3 instantiate local set accumulators of types Edge and VERTEX . They will be used to store each vertex's adjacency info as well as which node in their graph acts as the leader node (the one that the graph will be named after). In the case of QM7b, the leader node holds the label, as well. The select statement on line 7 populates the result set and accumulators. The edges that in the edges accumulator will be used to reconstruct the adjacency matricies. Keep in mind that any filtering ( WHERE or IF/ELSE ) can be used to more precicely choose which part(s) of the graph are returned. If the graph is heterogenious and the query requires multiple hops, you can simply add s.@edges += e to the ACCUM of any select statement to use that part of the graph in the model. The partitioning is done at the end to be sure that every vertex that's returned in the partitioned result set has its graph membership Write a subclass for Tiger2GNN \u00b6 subclass \u00b6 compute_vertex_vectors \u00b6 assemble/split \u00b6 DGL \u00b6 DGL Dataset \u00b6 DGL NN \u00b6","title":"Discrete Graph - Regression"},{"location":"examples/discrete-graph/#discrete-graph-regression","text":"","title":"Discrete Graph - Regression"},{"location":"examples/discrete-graph/#overview","text":"Graph-level regression on the QM7b dataset from TigerGraph using g2gnn. QM7b is a dataset for multitask learning. Since this is just an example, only one of those tasks is trained. The dataset is a homogeneous graph of 7,211 separate molucules where each vertex is one atom and each edge is a bond. The example code is HERE . Details are below.","title":"Overview"},{"location":"examples/discrete-graph/#write-the-query","text":"The query, found at the bottom of setup.gsql . It's responsible for gathering the graphy info that the model will train on. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 CREATE QUERY QM7bData ( INT total_partitions = 0 , INT current_partition ) FOR GRAPH QM7b { SetAccum < Edge > @ edges ; MaxAccum < VERTEX > @ lead_node ; AtomSrc = { Atom . * } ; AtomResult = SELECT s FROM AtomSrc : s - ( _ : e ) -> Atom : t ACCUM s . @ edges += e , IF s . label . size () > 0 THEN // Then it is the leader node s . @ lead_node += s , t . @ lead_node += s END ; AtomResult = SELECT s FROM AtomResult : s WHERE vertex_to_int ( s ) % total_partitions == current_partition ; PRINT AtomResult ; } Lines 2 and 3 instantiate local set accumulators of types Edge and VERTEX . They will be used to store each vertex's adjacency info as well as which node in their graph acts as the leader node (the one that the graph will be named after). In the case of QM7b, the leader node holds the label, as well. The select statement on line 7 populates the result set and accumulators. The edges that in the edges accumulator will be used to reconstruct the adjacency matricies. Keep in mind that any filtering ( WHERE or IF/ELSE ) can be used to more precicely choose which part(s) of the graph are returned. If the graph is heterogenious and the query requires multiple hops, you can simply add s.@edges += e to the ACCUM of any select statement to use that part of the graph in the model. The partitioning is done at the end to be sure that every vertex that's returned in the partitioned result set has its graph membership","title":"Write the query"},{"location":"examples/discrete-graph/#write-a-subclass-for-tiger2gnn","text":"","title":"Write a subclass for Tiger2GNN"},{"location":"examples/discrete-graph/#subclass","text":"","title":"subclass"},{"location":"examples/discrete-graph/#compute_vertex_vectors","text":"","title":"compute_vertex_vectors"},{"location":"examples/discrete-graph/#assemblesplit","text":"","title":"assemble/split"},{"location":"examples/discrete-graph/#dgl","text":"","title":"DGL"},{"location":"examples/discrete-graph/#dgl-dataset","text":"","title":"DGL Dataset"},{"location":"examples/discrete-graph/#dgl-nn","text":"","title":"DGL NN"},{"location":"examples/ego-graph/","text":"Graph Classification \u00b6 Graph-level classification on the Twitch Ego Nets dataset from a TigerGraph DB using g2gnn and DGL. Overview \u00b6 Another option for learning graph structures is at the graph or subgraph level. While node classification takes the region around a node to classify that node, graph-level classification aims to classify the region itself (in the case of subgraph classification), or the graph as a whole (e.g., molecule classification). The example code is HERE . First, install g2gnn and navigate to the graph-level/twitch directory under examples. Details are below. Load the Data \u00b6 Once you have stood up your tigergraph instance[^1], you can generate the files to load the data to graph by running [prepare_graph.ipynb]. When it's done, copy the files to your env. You should be able to run [setup.gsql] in the gsql shell to generate the schema, TwitchEgos, load the data and finally install the query. The Query \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 CREATE QUERY TwitchEgosData ( INT current_partition = 0 , INT total_partitions = 10 ) FOR GRAPH TwitchEgos SYNTAX V2 { TYPEDEF TUPLE < INT from_id , INT to_id > Edges ; # GNN train type TYPEDEF TUPLE < INT id , STRING v_type > Vert ; # GNN train type # Accumulators for tracking edges and involved vertices MaxAccum < INT > @id ; MapAccum < INT , SetAccum < Edges >> @@edges ; MapAccum < INT , SetAccum < Vert >> @@graph ; Users = { User .*} ; Users = SELECT s FROM Users : s WHERE vertex_to_int ( s ) % total_partitions == current_partition ACCUM s . @id += getvid ( s ), @@graph += ( getvid ( s ) -> Vert ( getvid ( s ), s . type )); # max 2-hop graph (this should be enough to encompas all subgraphs anyway) Friends1 = SELECT u FROM Users : s -( FRIENDSHIP : e )- Friend : u WHERE s . label > = 0 ACCUM u . @id += getvid ( u ), @@edges += ( getvid ( s ) -> Edges ( getvid ( s ), getvid ( u ))), @@graph += ( getvid ( s ) -> Vert ( getvid ( u ), u . type )); Friends2 = SELECT tgt FROM Users : s -( FRIENDSHIP : e )- _ : u -( FRIENDSHIP : e )- Friend : tgt WHERE s . label > = 0 ACCUM tgt . @id += tgt . id , @@edges += ( s . id -> Edges ( u . id , tgt . id )), @@graph += ( s . id -> Vert ( tgt . id , tgt . type )); @@edges += ( getvid ( s ) -> Edges ( getvid ( u ), getvid ( tgt ))), @@graph += ( getvid ( s ) -> Vert ( getvid ( tgt ), tgt . type )); Friends = Friends1 UNION Friends2 ; PRINT @@edges AS _ edges ; PRINT @@graph AS _ graph ; PRINT Users ; PRINT Friends ; } Lines 2 and 3 define types that graph2gnn will use to extract the graph structures from the query's response. These types allow for the minimal amount of data to be sent in the response. If you there are edge features in your dataset, add them as attributes to th Edges type like this: TYPEDEF TUPLE <INT from_id, INT to_id, INT feat_name1, String INT feat_name2...> Edges; Lines 6 through 8 instantiate global map accumulators to keep track of the adjacency information and graph membership. @@edges is a key-value store that holds the adjacency information of each subgraph where the ID of the subgraph's ego is the key and the values are the edges of that subgraph. @@graph has the same keys as edges, but its values simply store which vertices belong to which graph. This allows you to more quickly select features that go into the individual graphs when you're assembling the dataset. Lines 10 through 27 are the main body of the query. The actions to take note of are additions to the @@edges and @@graph map accums. Same as in the node-level query, you have access to all of the features of GSQL here to help you refine the subgraphs that you want. NOTE: It's critical that the edges accumulator is printed as _edges and the graph accumulator as _graph (lines 33, 34). Graph2GNN looks for _edges and _graph to reconstruct the adjecency information. In the case of subgraph classification, it's not uncommon that a vertex is a part of multiple graphs. Write a subclass for Tiger2GNN \u00b6 At this point, you should have a the TwitchEgos graph loaded to your TG instance. Now we'll begin to walk through the python code in graph-classification.ipynb to get the data and train a GCN. Feel free to write to write your own code and use the docs and example code as a guide. The first cell initializes Twitch2GNN, which is subclassed from Tiger2GNN. Subclassing is a convenient way to organize all the code around gathering data from the graph and making it ready for training. When you initialize the object from your subclass (or just the Tiger2GNN class), you must pass in the host, graph_name, the query you will call and any credential info (e.g. the restpp token). class Twitch2GNN ( Tiger2GNN ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) ... def compute_vertex_vectors ( self ): ... tg = Twitch2GNN ( host = f \"https:// { host } \" , graph_name = \"Cora\" , query = \"CoraData\" , token = tkn , ) Subgraph Query Method \u00b6 The call_subgraph_query calls a query written in the form described above. If there are any parameters, such as partitioning values, pass them in to the params argument. Call the method to get the data. tg . call_subgraph_query () Note \u00b6 At this point, g2gnn has done its job. You can use the resulting data in any way you wish! What follows is a guide to build the output from call_singlegraph_query into a GCN. Exploratory Data Analysis \u00b6 Once the data has been stored locally, if the data is still raw, now is when you would do any necessary exploratory data analysis (EDA), feature selection, etc. No EDA is needed for this example, since the graph stores the features directly. When the method is finished, the tg response directory will contain the following structure. project-root \u251c\u2500\u2500 tgresponse/ \u2502 \u251c\u2500\u2500 subgraph/ \u2502 \u2502 \u251c\u2500\u2500 <graph ID>_edges.csv (adjacency info... will become a model input) \u2502 \u2502 \u2514\u2500\u2500 <graph ID>_graph.csv (membership info... use to help select vertex vectors for the graph when assembling the datset) \u2502 \u251c\u2500\u2500 parts/ \u2502 \u2502 \u2514\u2500\u2500 partition_0.json (whole query responses, saved by partition) \u2502 \u251c\u2500\u2500 Friends.json (vertex set response... create vectors from these files) \u2502 \u251c\u2500\u2500 Users.json \u2514\u2500\u2500 graph-classification.ipynb compute_vertex_vectors \u00b6 The first of the custom methods in the Twitch2GNN subclass is used to compute, or in this case organize, the vectors that will represent each vertex. tg . compute_vertex_vectors () assemble/split \u00b6 The next cell shows the label split and some other dataset metrics. Here's a good place to have a sanity check. Unlike the node-level example, the assembly code is moved into a separate script in order to take advantage of concurrent processing. When the method is finished, the tg response directory will contain the following structure. project-root \u251c\u2500\u2500 tgresponse/ \u2502 \u251c\u2500\u2500 subgraph/ \u2502 \u2502 \u251c\u2500\u2500 <graph ID>_edges.csv (adjacency info... will become a model input) \u2502 \u2502 \u251c\u2500\u2500 <graph ID>_graph.csv (membership info... use to help select vertex vectors for the graph when assembling the datset) \u2502 \u2502 \u2514\u2500\u2500 <graph ID>_vectors.csv ***(vectorized vertices... will become a model input) \u2502 \u251c\u2500\u2500 parts/ \u2502 \u2502 \u2514\u2500\u2500 partition_0.json \u2502 \u251c\u2500\u2500 Friends.json \u2502 \u251c\u2500\u2500 Friends.csv \u2502 \u251c\u2500\u2500 Users.json \u2502 \u251c\u2500\u2500 Users.csv \u2514\u2500\u2500 graph-classification.ipynb DGL \u00b6 Now the graph data is ready. From this point on, feel free to continue in whichever graph-ML library is your preference. this example continues in pytorch-flavored DGL . DGL Dataset \u00b6 Following the dgl doc's for making your own dataset , we subclass DGLDataset class and override the __getitem__ , __len__ , and process methods. When constructing the graphs, keep in mind that node IDs have to be 0-indexed. Any node IDs used previously need to be tranlated into the ID-space for DGL. This is handled on lines 38-47 in the dataset instantiating cell. class TwitchDataset ( DGLDataset ): def __init__ ( self , num_nodes , ** kwargs ): super () . __init__ ( ** kwargs ) def __getitem__ ( self , i ): return self . graphs [ i ], self . labels [ i ] def __len__ ( self ): return len ( self . graphs ) def process ( self ): # lines 38-47 have the ID transfer code ... Model building and training \u00b6 Again, following DGL's docs , the model is constructed and trained in the following cells. class TwitchModel ( nn . Module ): def __init__ ( self , in_dim , hidden_dim , n_classes ): super () . __init__ () self . conv1 = dglnn . GraphConv ( in_dim , hidden_dim ) self . conv2 = dglnn . GraphConv ( hidden_dim , n_classes ) def forward ( self , g : dgl . DGLGraph , h ): h = self . conv1 ( g , h ) h = F . relu ( h ) h = self . conv2 ( g , h ) g . ndata [ 'h' ] = h return dgl . mean_nodes ( g , 'h' ) Conclusion \u00b6 Congrats on getting to the end! If you run into any issues or questions, please reach out on Github .","title":"Graph-Level"},{"location":"examples/ego-graph/#graph-classification","text":"Graph-level classification on the Twitch Ego Nets dataset from a TigerGraph DB using g2gnn and DGL.","title":"Graph Classification"},{"location":"examples/ego-graph/#overview","text":"Another option for learning graph structures is at the graph or subgraph level. While node classification takes the region around a node to classify that node, graph-level classification aims to classify the region itself (in the case of subgraph classification), or the graph as a whole (e.g., molecule classification). The example code is HERE . First, install g2gnn and navigate to the graph-level/twitch directory under examples. Details are below.","title":"Overview"},{"location":"examples/ego-graph/#load-the-data","text":"Once you have stood up your tigergraph instance[^1], you can generate the files to load the data to graph by running [prepare_graph.ipynb]. When it's done, copy the files to your env. You should be able to run [setup.gsql] in the gsql shell to generate the schema, TwitchEgos, load the data and finally install the query.","title":"Load the Data"},{"location":"examples/ego-graph/#the-query","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 CREATE QUERY TwitchEgosData ( INT current_partition = 0 , INT total_partitions = 10 ) FOR GRAPH TwitchEgos SYNTAX V2 { TYPEDEF TUPLE < INT from_id , INT to_id > Edges ; # GNN train type TYPEDEF TUPLE < INT id , STRING v_type > Vert ; # GNN train type # Accumulators for tracking edges and involved vertices MaxAccum < INT > @id ; MapAccum < INT , SetAccum < Edges >> @@edges ; MapAccum < INT , SetAccum < Vert >> @@graph ; Users = { User .*} ; Users = SELECT s FROM Users : s WHERE vertex_to_int ( s ) % total_partitions == current_partition ACCUM s . @id += getvid ( s ), @@graph += ( getvid ( s ) -> Vert ( getvid ( s ), s . type )); # max 2-hop graph (this should be enough to encompas all subgraphs anyway) Friends1 = SELECT u FROM Users : s -( FRIENDSHIP : e )- Friend : u WHERE s . label > = 0 ACCUM u . @id += getvid ( u ), @@edges += ( getvid ( s ) -> Edges ( getvid ( s ), getvid ( u ))), @@graph += ( getvid ( s ) -> Vert ( getvid ( u ), u . type )); Friends2 = SELECT tgt FROM Users : s -( FRIENDSHIP : e )- _ : u -( FRIENDSHIP : e )- Friend : tgt WHERE s . label > = 0 ACCUM tgt . @id += tgt . id , @@edges += ( s . id -> Edges ( u . id , tgt . id )), @@graph += ( s . id -> Vert ( tgt . id , tgt . type )); @@edges += ( getvid ( s ) -> Edges ( getvid ( u ), getvid ( tgt ))), @@graph += ( getvid ( s ) -> Vert ( getvid ( tgt ), tgt . type )); Friends = Friends1 UNION Friends2 ; PRINT @@edges AS _ edges ; PRINT @@graph AS _ graph ; PRINT Users ; PRINT Friends ; } Lines 2 and 3 define types that graph2gnn will use to extract the graph structures from the query's response. These types allow for the minimal amount of data to be sent in the response. If you there are edge features in your dataset, add them as attributes to th Edges type like this: TYPEDEF TUPLE <INT from_id, INT to_id, INT feat_name1, String INT feat_name2...> Edges; Lines 6 through 8 instantiate global map accumulators to keep track of the adjacency information and graph membership. @@edges is a key-value store that holds the adjacency information of each subgraph where the ID of the subgraph's ego is the key and the values are the edges of that subgraph. @@graph has the same keys as edges, but its values simply store which vertices belong to which graph. This allows you to more quickly select features that go into the individual graphs when you're assembling the dataset. Lines 10 through 27 are the main body of the query. The actions to take note of are additions to the @@edges and @@graph map accums. Same as in the node-level query, you have access to all of the features of GSQL here to help you refine the subgraphs that you want. NOTE: It's critical that the edges accumulator is printed as _edges and the graph accumulator as _graph (lines 33, 34). Graph2GNN looks for _edges and _graph to reconstruct the adjecency information. In the case of subgraph classification, it's not uncommon that a vertex is a part of multiple graphs.","title":"The Query"},{"location":"examples/ego-graph/#write-a-subclass-for-tiger2gnn","text":"At this point, you should have a the TwitchEgos graph loaded to your TG instance. Now we'll begin to walk through the python code in graph-classification.ipynb to get the data and train a GCN. Feel free to write to write your own code and use the docs and example code as a guide. The first cell initializes Twitch2GNN, which is subclassed from Tiger2GNN. Subclassing is a convenient way to organize all the code around gathering data from the graph and making it ready for training. When you initialize the object from your subclass (or just the Tiger2GNN class), you must pass in the host, graph_name, the query you will call and any credential info (e.g. the restpp token). class Twitch2GNN ( Tiger2GNN ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) ... def compute_vertex_vectors ( self ): ... tg = Twitch2GNN ( host = f \"https:// { host } \" , graph_name = \"Cora\" , query = \"CoraData\" , token = tkn , )","title":"Write a subclass for Tiger2GNN"},{"location":"examples/ego-graph/#subgraph-query-method","text":"The call_subgraph_query calls a query written in the form described above. If there are any parameters, such as partitioning values, pass them in to the params argument. Call the method to get the data. tg . call_subgraph_query ()","title":"Subgraph Query Method"},{"location":"examples/ego-graph/#note","text":"At this point, g2gnn has done its job. You can use the resulting data in any way you wish! What follows is a guide to build the output from call_singlegraph_query into a GCN.","title":"Note"},{"location":"examples/ego-graph/#exploratory-data-analysis","text":"Once the data has been stored locally, if the data is still raw, now is when you would do any necessary exploratory data analysis (EDA), feature selection, etc. No EDA is needed for this example, since the graph stores the features directly. When the method is finished, the tg response directory will contain the following structure. project-root \u251c\u2500\u2500 tgresponse/ \u2502 \u251c\u2500\u2500 subgraph/ \u2502 \u2502 \u251c\u2500\u2500 <graph ID>_edges.csv (adjacency info... will become a model input) \u2502 \u2502 \u2514\u2500\u2500 <graph ID>_graph.csv (membership info... use to help select vertex vectors for the graph when assembling the datset) \u2502 \u251c\u2500\u2500 parts/ \u2502 \u2502 \u2514\u2500\u2500 partition_0.json (whole query responses, saved by partition) \u2502 \u251c\u2500\u2500 Friends.json (vertex set response... create vectors from these files) \u2502 \u251c\u2500\u2500 Users.json \u2514\u2500\u2500 graph-classification.ipynb","title":"Exploratory Data Analysis"},{"location":"examples/ego-graph/#compute_vertex_vectors","text":"The first of the custom methods in the Twitch2GNN subclass is used to compute, or in this case organize, the vectors that will represent each vertex. tg . compute_vertex_vectors ()","title":"compute_vertex_vectors"},{"location":"examples/ego-graph/#assemblesplit","text":"The next cell shows the label split and some other dataset metrics. Here's a good place to have a sanity check. Unlike the node-level example, the assembly code is moved into a separate script in order to take advantage of concurrent processing. When the method is finished, the tg response directory will contain the following structure. project-root \u251c\u2500\u2500 tgresponse/ \u2502 \u251c\u2500\u2500 subgraph/ \u2502 \u2502 \u251c\u2500\u2500 <graph ID>_edges.csv (adjacency info... will become a model input) \u2502 \u2502 \u251c\u2500\u2500 <graph ID>_graph.csv (membership info... use to help select vertex vectors for the graph when assembling the datset) \u2502 \u2502 \u2514\u2500\u2500 <graph ID>_vectors.csv ***(vectorized vertices... will become a model input) \u2502 \u251c\u2500\u2500 parts/ \u2502 \u2502 \u2514\u2500\u2500 partition_0.json \u2502 \u251c\u2500\u2500 Friends.json \u2502 \u251c\u2500\u2500 Friends.csv \u2502 \u251c\u2500\u2500 Users.json \u2502 \u251c\u2500\u2500 Users.csv \u2514\u2500\u2500 graph-classification.ipynb","title":"assemble/split"},{"location":"examples/ego-graph/#dgl","text":"Now the graph data is ready. From this point on, feel free to continue in whichever graph-ML library is your preference. this example continues in pytorch-flavored DGL .","title":"DGL"},{"location":"examples/ego-graph/#dgl-dataset","text":"Following the dgl doc's for making your own dataset , we subclass DGLDataset class and override the __getitem__ , __len__ , and process methods. When constructing the graphs, keep in mind that node IDs have to be 0-indexed. Any node IDs used previously need to be tranlated into the ID-space for DGL. This is handled on lines 38-47 in the dataset instantiating cell. class TwitchDataset ( DGLDataset ): def __init__ ( self , num_nodes , ** kwargs ): super () . __init__ ( ** kwargs ) def __getitem__ ( self , i ): return self . graphs [ i ], self . labels [ i ] def __len__ ( self ): return len ( self . graphs ) def process ( self ): # lines 38-47 have the ID transfer code ...","title":"DGL Dataset"},{"location":"examples/ego-graph/#model-building-and-training","text":"Again, following DGL's docs , the model is constructed and trained in the following cells. class TwitchModel ( nn . Module ): def __init__ ( self , in_dim , hidden_dim , n_classes ): super () . __init__ () self . conv1 = dglnn . GraphConv ( in_dim , hidden_dim ) self . conv2 = dglnn . GraphConv ( hidden_dim , n_classes ) def forward ( self , g : dgl . DGLGraph , h ): h = self . conv1 ( g , h ) h = F . relu ( h ) h = self . conv2 ( g , h ) g . ndata [ 'h' ] = h return dgl . mean_nodes ( g , 'h' )","title":"Model building and training"},{"location":"examples/ego-graph/#conclusion","text":"Congrats on getting to the end! If you run into any issues or questions, please reach out on Github .","title":"Conclusion"},{"location":"examples/examples/","text":"Example problems to show g2gnn in action across a few different problem spaces Single-Graph \u00b6 Node-Level Classification with the Cora benchmark dataset Subgraph (Ego-Centric) Classification \u00b6 Graph-Level Classification with the Twitch Ego Nets dataset","title":"Examples"},{"location":"examples/examples/#single-graph","text":"Node-Level Classification with the Cora benchmark dataset","title":"Single-Graph"},{"location":"examples/examples/#subgraph-ego-centric-classification","text":"Graph-Level Classification with the Twitch Ego Nets dataset","title":"Subgraph (Ego-Centric) Classification"},{"location":"examples/node-level/","text":"Node Classification \u00b6 Node classification on the Cora benchmark dataset from a TigerGraph DB using g2gnn and DGL. Overview \u00b6 Node classification, especially with the Cora dataset, is a common \"Hello, world\" for GNNs. The dataset is a single, homogenious graph of 2708 academic papers on 7 different topics. The edges between the papers represent one paper citing the works of the other. The task is to use the contents of each paper (one-hot encoded word-vectors) and the graph to determine which topic the paper is about. The example code is HERE . First, install g2gnn and navigate to the node-level/cora directory under examples. Details are below. Load the Data \u00b6 Once you have stood up your tigergraph instance 1 , you can generate the files to load the data to graph by running prepare_graph.ipynb . When it's done, copy the files to your env. You should be able to run setup.gsql in the gsql shell to generate the schema, CoraGraph, load the data and finally install the query. The Query \u00b6 The query, found at the bottom of setup.gsql , is responsible for gathering the data and the graphy info that the model will train on. 1 2 3 4 5 6 7 8 9 10 CREATE QUERY CoraData () FOR GRAPH Cora { SetAccum < Edge > @@edges ; PaperSrc = { Paper .*} ; PaperResult = SELECT s FROM PaperSrc : s -( CITES : e )- > Paper : tgt ACCUM @@edges += e ; PRINT @@edges as _ edges ; PRINT PaperSrc ; } Line 2 instantiates a global set accumulator of type Edge and line 6 begins to populate the accumulator. The edges that are in this accumulator will be used to reconstruct the graph's adjacency matrix. Keep in mind that any filtering ( WHERE or IF/ELSE ) can be used to more precicely choose which parts of the graph will be used. Since this is not a large graph, it can safely be extracted in one call (no need for partitioning). If the graph is heterogenious and the query requires multiple hops, you can simply add @@edges += e to the ACCUM of any select statement to use that part of the graph in the model. NOTE: It's critical that the edges accumulator is printed as _edges (line 8). Graph2GNN looks for _edges to reconstruct the adjecency information. FOR LARGE GRAPHS: Graph2GNN is fully compatible with partioned queries. Replace the beginning of the query with the following lines to return smaller chunks that g2gnn will piece back together. 1 2 3 4 5 6 CREATE QUERY CoraData ( INT total_partitions = 0 , INT current_partition ) FOR GRAPH Cora { SetAccum < Edge > @@edges ; PaperSrc = { Paper .*} ; PaperSrc = SELECT s FROM PaperSrc : s WHERE vertex_to_int ( s ) % total_partitions == current_partition ; ... Write a subclass for Tiger2GNN \u00b6 At this point, you should have a the Cora citation graph loaded to your TG instance. Now we'll begin to walk through the python code in node-classification.ipynb to get the data and train a 2-hop GCN. Feel free to write to write your own code and use the docs and example code as a guide. The first cell initializes Cora2GNN, which is subclassed from Tiger2GNN. Subclassing is a convenient way to organize all the code around gathering data from the graph and making it ready for training. When you initialize the object from your subclass (or just the Tiger2GNN class), you must pass in the host, graph_name, the query you will call and any credential info (e.g. the restpp token). class Cora2GNN ( Tiger2GNN ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) ... def compute_vertex_vectors ( self ): ... def assemble_data ( self , samples_per_class ): ... tg = Cora2GNN ( host = f \"https:// { host } \" , graph_name = \"Cora\" , query = \"CoraData\" , token = tkn , ) Singlegraph Query Method \u00b6 The call_singlegraph_query calls a query written in the form described above. If there are any parameters, such as partitioning values, pass them in to the params argument. Call the method to get the data. tg . call_singlegraph_query () Note \u00b6 At this point, g2gnn has done its job. You can use the resulting data in any way you wish! What follows is a guide to build the output from call_singlegraph_query into a GCN. Exploratory Data Analysis \u00b6 Once the data has been stored locally, if the data is still raw, now is when you would do any necessary exploratory data analysis (EDA), feature selection, etc. No EDA is needed for this example, since the graph stores the features directly. When the method is finished, the tg response directory will contain the following structure. project-root \u2514\u2500\u2500 tgresponse/ \u251c\u2500\u2500 subgraph/ (empty dir) \u251c\u2500\u2500 parts/ \u2502 \u2514\u2500\u2500 partition_0.json (whole query responses, saved by partition) \u251c\u2500\u2500 PaperSrc.json (vertex set response... create vectors from these files) \u251c\u2500\u2500 edges.csv *(adjacency info... will become a model input) \u2514\u2500\u2500 node-classification.ipynb compute_vertex_vectors \u00b6 The first of the custom methods in the Cora2GNN subclass is used to compute, or in this case organize, the vectors that will represent each vertex. tg . compute_vertex_vectors () assemble/split \u00b6 The next cell shows the label split and some other dataset metrics. Here's a good place to have a sanity check -- there should be 2708 total labels since there are 2708 nodes in the graph. Within this cell, the second custom method of the Cora2GNN subclass is called. In this notebook, assemble_data organizes the datset into train, eval and test datasets. tg . assemble_data ( samples_per_class = samples_per_class ) DGL \u00b6 Now the graph data is ready. From this point on, feel free to continue in whichever graph-ML library is your preference. this example continues in pytorch-flavored DGL . DGL Dataset \u00b6 Following the dgl doc's for making your own dataset , we subclass DGLDataset class and override the __getitem__ , __len__ , and process methods. When constructing the graph, keep in mind that node IDs have to be 0-indexed. Any node IDs used previously need to be tranlated into the ID-space for DGL. This is handled in prepare_graph.ipynb , but you will need to do this if you use DGL for your own solutions. An example can be seen in the graph-level docs ( graph-classification.ipynb ) class CoraDataset ( DGLDataset ): def __init__ ( self , num_nodes , ** kwargs ): super () . __init__ ( ** kwargs ) def __len__ ( self ): return 1 def __getitem__ ( self , idx ): return self . graph def process ( self ): ... Model building and training \u00b6 Again, following DGL's docs and Kipf's GCN paper , the model is constructed and trained in the following cells. class GCN ( nn . Module ): def __init__ ( self , in_feats , h_feats , num_classes , drop = 0.5 ): super () . __init__ () self . conv1 = GraphConv ( in_feats , h_feats ) self . conv2 = GraphConv ( h_feats , num_classes ) self . d1 = nn . Dropout ( drop ) def forward ( self , g , in_feat ): h = F . dropout ( in_feat ) h = self . conv1 ( g , h ) h = F . relu ( h ) h = self . conv2 ( g , h ) return h Conclusion \u00b6 If all went well, your model should have a test accuracy of ~83%. Congrats on getting to the end! A lot of problems can be modeled as a single graph where node classification or link prediction is the task. If your problem is better suited as graph-level predictions, check out the graph-level g2gnn example module. https://www.tigergraph.com/get-tigergraph/ \u21a9","title":"Node-Level"},{"location":"examples/node-level/#node-classification","text":"Node classification on the Cora benchmark dataset from a TigerGraph DB using g2gnn and DGL.","title":"Node Classification"},{"location":"examples/node-level/#overview","text":"Node classification, especially with the Cora dataset, is a common \"Hello, world\" for GNNs. The dataset is a single, homogenious graph of 2708 academic papers on 7 different topics. The edges between the papers represent one paper citing the works of the other. The task is to use the contents of each paper (one-hot encoded word-vectors) and the graph to determine which topic the paper is about. The example code is HERE . First, install g2gnn and navigate to the node-level/cora directory under examples. Details are below.","title":"Overview"},{"location":"examples/node-level/#load-the-data","text":"Once you have stood up your tigergraph instance 1 , you can generate the files to load the data to graph by running prepare_graph.ipynb . When it's done, copy the files to your env. You should be able to run setup.gsql in the gsql shell to generate the schema, CoraGraph, load the data and finally install the query.","title":"Load the Data"},{"location":"examples/node-level/#the-query","text":"The query, found at the bottom of setup.gsql , is responsible for gathering the data and the graphy info that the model will train on. 1 2 3 4 5 6 7 8 9 10 CREATE QUERY CoraData () FOR GRAPH Cora { SetAccum < Edge > @@edges ; PaperSrc = { Paper .*} ; PaperResult = SELECT s FROM PaperSrc : s -( CITES : e )- > Paper : tgt ACCUM @@edges += e ; PRINT @@edges as _ edges ; PRINT PaperSrc ; } Line 2 instantiates a global set accumulator of type Edge and line 6 begins to populate the accumulator. The edges that are in this accumulator will be used to reconstruct the graph's adjacency matrix. Keep in mind that any filtering ( WHERE or IF/ELSE ) can be used to more precicely choose which parts of the graph will be used. Since this is not a large graph, it can safely be extracted in one call (no need for partitioning). If the graph is heterogenious and the query requires multiple hops, you can simply add @@edges += e to the ACCUM of any select statement to use that part of the graph in the model. NOTE: It's critical that the edges accumulator is printed as _edges (line 8). Graph2GNN looks for _edges to reconstruct the adjecency information. FOR LARGE GRAPHS: Graph2GNN is fully compatible with partioned queries. Replace the beginning of the query with the following lines to return smaller chunks that g2gnn will piece back together. 1 2 3 4 5 6 CREATE QUERY CoraData ( INT total_partitions = 0 , INT current_partition ) FOR GRAPH Cora { SetAccum < Edge > @@edges ; PaperSrc = { Paper .*} ; PaperSrc = SELECT s FROM PaperSrc : s WHERE vertex_to_int ( s ) % total_partitions == current_partition ; ...","title":"The Query"},{"location":"examples/node-level/#write-a-subclass-for-tiger2gnn","text":"At this point, you should have a the Cora citation graph loaded to your TG instance. Now we'll begin to walk through the python code in node-classification.ipynb to get the data and train a 2-hop GCN. Feel free to write to write your own code and use the docs and example code as a guide. The first cell initializes Cora2GNN, which is subclassed from Tiger2GNN. Subclassing is a convenient way to organize all the code around gathering data from the graph and making it ready for training. When you initialize the object from your subclass (or just the Tiger2GNN class), you must pass in the host, graph_name, the query you will call and any credential info (e.g. the restpp token). class Cora2GNN ( Tiger2GNN ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) ... def compute_vertex_vectors ( self ): ... def assemble_data ( self , samples_per_class ): ... tg = Cora2GNN ( host = f \"https:// { host } \" , graph_name = \"Cora\" , query = \"CoraData\" , token = tkn , )","title":"Write a subclass for Tiger2GNN"},{"location":"examples/node-level/#singlegraph-query-method","text":"The call_singlegraph_query calls a query written in the form described above. If there are any parameters, such as partitioning values, pass them in to the params argument. Call the method to get the data. tg . call_singlegraph_query ()","title":"Singlegraph Query Method"},{"location":"examples/node-level/#note","text":"At this point, g2gnn has done its job. You can use the resulting data in any way you wish! What follows is a guide to build the output from call_singlegraph_query into a GCN.","title":"Note"},{"location":"examples/node-level/#exploratory-data-analysis","text":"Once the data has been stored locally, if the data is still raw, now is when you would do any necessary exploratory data analysis (EDA), feature selection, etc. No EDA is needed for this example, since the graph stores the features directly. When the method is finished, the tg response directory will contain the following structure. project-root \u2514\u2500\u2500 tgresponse/ \u251c\u2500\u2500 subgraph/ (empty dir) \u251c\u2500\u2500 parts/ \u2502 \u2514\u2500\u2500 partition_0.json (whole query responses, saved by partition) \u251c\u2500\u2500 PaperSrc.json (vertex set response... create vectors from these files) \u251c\u2500\u2500 edges.csv *(adjacency info... will become a model input) \u2514\u2500\u2500 node-classification.ipynb","title":"Exploratory Data Analysis"},{"location":"examples/node-level/#compute_vertex_vectors","text":"The first of the custom methods in the Cora2GNN subclass is used to compute, or in this case organize, the vectors that will represent each vertex. tg . compute_vertex_vectors ()","title":"compute_vertex_vectors"},{"location":"examples/node-level/#assemblesplit","text":"The next cell shows the label split and some other dataset metrics. Here's a good place to have a sanity check -- there should be 2708 total labels since there are 2708 nodes in the graph. Within this cell, the second custom method of the Cora2GNN subclass is called. In this notebook, assemble_data organizes the datset into train, eval and test datasets. tg . assemble_data ( samples_per_class = samples_per_class )","title":"assemble/split"},{"location":"examples/node-level/#dgl","text":"Now the graph data is ready. From this point on, feel free to continue in whichever graph-ML library is your preference. this example continues in pytorch-flavored DGL .","title":"DGL"},{"location":"examples/node-level/#dgl-dataset","text":"Following the dgl doc's for making your own dataset , we subclass DGLDataset class and override the __getitem__ , __len__ , and process methods. When constructing the graph, keep in mind that node IDs have to be 0-indexed. Any node IDs used previously need to be tranlated into the ID-space for DGL. This is handled in prepare_graph.ipynb , but you will need to do this if you use DGL for your own solutions. An example can be seen in the graph-level docs ( graph-classification.ipynb ) class CoraDataset ( DGLDataset ): def __init__ ( self , num_nodes , ** kwargs ): super () . __init__ ( ** kwargs ) def __len__ ( self ): return 1 def __getitem__ ( self , idx ): return self . graph def process ( self ): ...","title":"DGL Dataset"},{"location":"examples/node-level/#model-building-and-training","text":"Again, following DGL's docs and Kipf's GCN paper , the model is constructed and trained in the following cells. class GCN ( nn . Module ): def __init__ ( self , in_feats , h_feats , num_classes , drop = 0.5 ): super () . __init__ () self . conv1 = GraphConv ( in_feats , h_feats ) self . conv2 = GraphConv ( h_feats , num_classes ) self . d1 = nn . Dropout ( drop ) def forward ( self , g , in_feat ): h = F . dropout ( in_feat ) h = self . conv1 ( g , h ) h = F . relu ( h ) h = self . conv2 ( g , h ) return h","title":"Model building and training"},{"location":"examples/node-level/#conclusion","text":"If all went well, your model should have a test accuracy of ~83%. Congrats on getting to the end! A lot of problems can be modeled as a single graph where node classification or link prediction is the task. If your problem is better suited as graph-level predictions, check out the graph-level g2gnn example module. https://www.tigergraph.com/get-tigergraph/ \u21a9","title":"Conclusion"},{"location":"getting-started/getting-started/","text":"Subclassing (Under Construction) \u00b6 It's best practice to subclass Tiger2GNN and implement the methods in a way that's more custom to your use case Here's an example (todo needs commenting) from graph2gnn import Tiger2GNN import numpy as np from tqdm import tqdm import json class MyImplementation ( Tiger2GNN ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . response_set_keys = [] self . nlp = None self . num_labels = None self . labels = None self . output_path = 'subgraph' def compute_vertex_vectors ( self , key = 'VertexType' ): vectors = [] path = f ' { self . output_path } / { key } .json' vertex_list = json . load ( open ( path )) # embed using word vectorization for vert in tqdm ( vertex_list , desc = key ): vector = make_vector ( vert ) # turn the vertex json into a vector vectors . append ([ vert [ 'v_id' ], vector ]) # save computational progress save_arr = np . array ( vectors , dtype = object ) np . save ( f ' { self . output_path } /vectors/ { key } ' , save_arr ) def determine_labels ( self ): indvs = json . load ( open ( f ' { self . output_path } /Indv.json' )) labels = {} for i in tqdm ( indvs ): label = i [ 'attributes' ][ '@label' ] labels [ i [ 'v_id' ]] = label self . labels = labels ''' ------------------------------------------------ ''' # create the config object tg = MyImplementation ( host = 'https://your-graph-host' , graph_name = 'MyGraph' , token = '<bearer token>' , query = 'my_graph2gnn_query' , cert_path = 'path/to/cert.crt' ) resp = tg . call_singlegraph_query () tg . compute_adjacency_matxs () tg . compute_vertex_vectors () tg . determine_labels ()","title":"Getting started"},{"location":"getting-started/getting-started/#subclassing-under-construction","text":"It's best practice to subclass Tiger2GNN and implement the methods in a way that's more custom to your use case Here's an example (todo needs commenting) from graph2gnn import Tiger2GNN import numpy as np from tqdm import tqdm import json class MyImplementation ( Tiger2GNN ): def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . response_set_keys = [] self . nlp = None self . num_labels = None self . labels = None self . output_path = 'subgraph' def compute_vertex_vectors ( self , key = 'VertexType' ): vectors = [] path = f ' { self . output_path } / { key } .json' vertex_list = json . load ( open ( path )) # embed using word vectorization for vert in tqdm ( vertex_list , desc = key ): vector = make_vector ( vert ) # turn the vertex json into a vector vectors . append ([ vert [ 'v_id' ], vector ]) # save computational progress save_arr = np . array ( vectors , dtype = object ) np . save ( f ' { self . output_path } /vectors/ { key } ' , save_arr ) def determine_labels ( self ): indvs = json . load ( open ( f ' { self . output_path } /Indv.json' )) labels = {} for i in tqdm ( indvs ): label = i [ 'attributes' ][ '@label' ] labels [ i [ 'v_id' ]] = label self . labels = labels ''' ------------------------------------------------ ''' # create the config object tg = MyImplementation ( host = 'https://your-graph-host' , graph_name = 'MyGraph' , token = '<bearer token>' , query = 'my_graph2gnn_query' , cert_path = 'path/to/cert.crt' ) resp = tg . call_singlegraph_query () tg . compute_adjacency_matxs () tg . compute_vertex_vectors () tg . determine_labels ()","title":"Subclassing (Under Construction)"}]}